---
title: 眼动追踪推荐眼镜
date: 2019-09-19 19:09:09
tags:
---
<div align=center>
<img src = "眼动追踪推荐眼镜\000.png" >
</div>

<!-- more -->
<The rest of contents | 余下全文>

# SUMMARY

## WHY  

眼睛是人类面部最为显著的特征之一，眼睛及其运动在用户的认知、兴趣、情绪表达等过程中有着至关重要的作用。  

眼动追踪技术通过追踪人体眼球运动，来识别用户所处的环境，预测用户的需求及状态，并进行响应，以实现用眼睛控制设备、完成一些操作。

AR眼镜自诞生起就为人类更加直观地理解眼前的世界提供了诸多便利。结合眼动追踪技术，AR眼镜可以更加了解用户的兴趣点和需求，提供更精准的信息推荐，为用户带来更加自然的交互体验。


## WHAT

本文涉及一种人眼注视点的实时估计眼镜，该系统通过提取人眼信息，建立人眼与兴趣点映射关系，实现用户关注信息的实时推荐功能。

本文主要从以下三个方面进行研究：
* 首先是人眼关键点的检测问题，通过多级级联的回归树进行眼眶和通孔的检测，得到关键点特征信息；  
* 其次是兴趣点的估计问题，通过梯度提升回归算法建立人眼关键点和兴趣点的映射关系；  
* 最后是兴趣点处的目标识别问题，通过yolo目标检测网络对检测视野中的待推荐目标，判断兴趣点是否落在识别框中。

## HOW

step1. 眼镜上部署两个ip摄像头，camera1获取用户视野图像，camera2获取用户眼睛图像，图像实时回传至中央处理器cpu；  
step2.1 眼动信息处理，提取camera2中用户眼眶和眼球的关键点信息；  
step2.2 兴趣点估计，由step2中的关键点特征估算camera1中人眼注视点；  
step3. 用户视野图像目标检测，实时检测camera1中的待推荐目标，记录相应位置信息；  
step4. 识别与信息推荐，判断step2中计算的兴趣点是否落在step3中计算的识别框中，进行语音信息推荐。


<div align=center>
<img src = "眼动追踪推荐眼镜\detect.gif" width=500 height=150>
</div>


## 参考资料

* 一文读懂眼动追踪技术及应用  
https://mp.weixin.qq.com/s?src=11&timestamp=1569225062&ver=1869&signature=ML2J84rjyrwpwNWYIXxrqaJIq7YZdGRAfzd8*q-v2vtSxIpREhSF0MpcjruTuYEFfgRR15F9IfmkCg1iQ1HY4hZtzFGT3gOF8LGyVoxKTtr89LIuel9h6XbniBG2y-w0&new=1   



# 眼动追踪推荐眼镜一：硬件篇

硬件组成部分主要就是眼镜架、摄像头和供电系统。

可以参考的开源方案有
* Pupil Labs  
https://pupil-labs.com/


当前设计使用的是USB摄像头直接连接电脑的方案：240°鱼眼摄像头被固定在眼镜框正前方负责采集用户视野图像，720P摄像头则固定在右前方伸出的支架上负责采集右眼图像，摄像头通过USB与计算机连接完成图像传输。

以下是对ip摄像头的测试，计划下一步用于眼镜的改进。


<html>
<head>
<meta charset="utf-8"> 
<title>title_test</title> 
<script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js">
</script>
<script>
$(document).ready(function(){
    $(document).on('click', '.fold_hider', function(){
        $('>.fold', this.parentNode).slideToggle();
        $('>:first', this).toggleClass('open');
    });
    $("div.fold").css("display","none");
});
</script>
</head>
</html>

## ip摄像头

45元的WIFI摄像头模块，能玩出什么花样？ - 知乎  
https://zhuanlan.zhihu.com/p/60565132

ip摄像头选用ESP32-CAM模块，包括ESP32wifi模块和一个OV2640的200W像素摄像头。

### 烧录固件

硬件连接，注意需要将GPIO0和边上的GND短接进入烧录模式

| USB-TTL    |   ESP32-CAM    |
|  :----:    | :----:         |
| 3.3V       |   VCC          |
| GND        |   GND          |
| RX         |   U0T          |
| TX         |   U0R          |
|            |   IO0和GND短接  |

<img src = "眼动追踪推荐眼镜\01.jpg">

烧录工具我们用的是Arduino IDE，首先需要添加ESP32的芯片支持  
打开首选项，在附加开发板管理器网址里填上：https://dl.espressif.com/dl/package_esp32_index.json ，然后单击 好 。
<img src = "眼动追踪推荐眼镜\04.png">


然后在工具里打开开发板管理器，等待索引做完之后找到ESP32并安装。
<img src = "眼动追踪推荐眼镜\00.png">

在github下载示例代码。  
* RuiSantosdotme/arduino-esp32-CameraWebServer: CameraWebServer for ESP32-CAM in Arduino IDE  
https://github.com/RuiSantosdotme/arduino-esp32-CameraWebServer


打开CameraWebServer.ino文件，进行一些设置
``` c
// Select camera model
//#define CAMERA_MODEL_WROVER_KIT
//#define CAMERA_MODEL_M5STACK_PSRAM
#define CAMERA_MODEL_AI_THINKER

const char* ssid = "REPLACE_WITH_YOUR_SSID";
const char* password = "REPLACE_WITH_YOUR_PASSWORD";
```

在开发板里面选对我们需要的开发板和相关设置，然后点击上传就好。

<img src = "眼动追踪推荐眼镜\02.png">


### 测试

通过浏览器输入ESP32-CAM的IP地址，就能看到设置页面了。

点击Start Stream就能看到实时影像了。

通过python脚本读取图像。

<body>
<div>
    <div class="fold_hider">
        <div class="close hider_title">点击显示/隐藏代码</div>
    </div>
    <div class="fold">

```python  
import cv2
import urllib.request
import numpy as np
host = "192.168.0.100:81" # 在这里记得修改ip，否则是无法调用的，刚刚浏览器输入的地址
hoststr = 'http://' + host + '/stream'
print('Streaming ' + hoststr)
print('Print Esc to quit')
stream=urllib.request.urlopen(hoststr)
bytes=bytes()
while True:
    bytes+=stream.read(1024)
    a = bytes.find(b'\xff\xd8')
    b = bytes.find(b'\xff\xd9')
    i = 0
    if a!=-1 and b!=-1:
        jpg = bytes[a:b+2]
        bytes= bytes[b+2:]
        #flags = 1 for color image
        try:
            i = cv2.imdecode(np.fromstring(jpg, dtype=np.uint8),flags=1)
            # print i.shape
            cv2.imshow("wjw",i)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                exit(0)
        except Exception as e:
            print('保存失败', e)
```
</div>
</div>
</body>

<div align=center>
<img src = "眼动追踪推荐眼镜\06.png">
</div>


### 问题记录

* 编译项目报错 “sketch\app_httpd.cpp:23:20: fatal error: dl_lib.h: No such file or directory”

app_httpd.cpp 中注释相应库
``` c
//#include "dl_lib.h"
```

* 编译项目报错 “ "WiFi.h" 对应多个库
 已使用： C:\Users\Administrator\AppData\Local\Arduino15\packages\esp32\hardware\esp32\1.0.3\libraries\WiFi
 未使用：H:\Program Files\arduino-1.8.3\libraries\WiFi
exit status 1 ”

移除文件夹 H:\Program Files\arduino-1.8.3\libraries\WiFi

* 点击Start Stream没有反应。

<div align=center>
<img src = "眼动追踪推荐眼镜\03.png">
</div>
使用Chrome浏览器。

* 注意供电。


# 眼动追踪推荐眼镜二：软件篇

软件部分主要解决下面三个问题：

* 首先是人眼关键点的检测问题，通过多级级联的回归树进行眼眶和通孔的检测，得到关键点特征信息；  
* 其次是兴趣点的估计问题，通过梯度提升回归算法建立人眼关键点和兴趣点的映射关系；  
* 最后是兴趣点处的目标识别问题，通过yolo目标检测网络对检测视野中的待推荐目标，判断兴趣点是否落在识别框中。


## 人眼关键点检测

首先使用人眼摄像头采集人眼图像数据，然后使用dlib自带的image数据库标注工具制作数据集，最后通过dlib的官方代码训练出模型，并对模型进行测试。

模型分别对眼眶和眼珠进行识别，并检测它们的位置。

* dlib train_object_detector  
http://dlib.net/train_object_detector.py.html

``` python
import os
import sys
import glob
import dlib

options = dlib.simple_object_detector_training_options()
# 单个眼睛不是左右对称的
# options.add_left_right_image_flips = True
# 支持向量机的C参数，通常默认取为5.自己适当更改参数以达到最好的效果
options.C = 5
# 线程数，你电脑有4核的话就填4
options.num_threads = 4
options.be_verbose = True

training_xml_path = "pupil.xml"
dlib.train_simple_object_detector(training_xml_path, "pupil.svm", options)
print("Training accuracy: {}".format(
    dlib.test_simple_object_detector(training_xml_path, "pupil.svm")))

training_xml_path = "eye.xml"
dlib.train_simple_object_detector(training_xml_path, "eye.svm", options)
print("Training accuracy: {}".format(
    dlib.test_simple_object_detector(training_xml_path, "eye.svm")))

```

## 兴趣点映射估计

分别采集人眼图像中眼眶和眼珠的坐标数据和视野图像中目标点的坐标数据，通过sklearn中的梯度提升回归算法建立人眼关键点数据和兴趣点数据的映射关系。  

``` python
# 读取数据
point = pd.read_csv("csv_data/points_p.csv")
world_x = point["world_x"]
world_y = point["world_y"]

# 读取数据中的标签列
eye = point[['eye_x', 'eye_y', 'pipil_x', 'pupil_y', 'pupil_w', 'pupil_h']]
print(eye)

clf = GBR(max_depth=10)
# clf = SGDR(loss='huber',penalty='l2',alpha=0.9,max_iter=1000)
# clf = KNeighborsRegressor(n_neighbors=20, weights="distance", algorithm="ball_tree", leaf_size=50)

clf.fit(eye, world_x)
joblib.dump(clf, "model/world_x.pkl")
print('得分：',clf.score(eye, world_x))

clf.fit(eye, world_y)
joblib.dump(clf, "model/world_y.pkl")
print('得分：',clf.score(eye, world_y))

```

## 目标检测

### yolov3

制作voc格式数据集，使用keras版本yolov3训练识别模型

### ImageAI

使用ImageAI提供的方法对视野图像进行目标检测。

``` python
# coding:utf-8
#  imageai下载地址：https://github.com/OlafenwaMoses/ImageAI
#  resnet50_coco_best_v2.1.0.h5 模型下载地址：https://github.com/fizyr/keras-retinanet/releases/
from imageai.Detection import ObjectDetection  # 导入了 ImageAI 目标检测类
import cv2
import os
import time
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import matplotlib.pyplot as plt

def targetDetection(imgArray,model_path):
    """
    :param imgArray: 图片数据，类型为ndarray
    :param model_path: retinanet模型路径
    :return:
    """
    path = os.path.abspath(model_path)
    detector = ObjectDetection()  # 定义了目标检测类
    detector.setModelTypeAsRetinaNet()  # 模型的类型设置为 RetinaNet
    detector.setModelPath(path)  # 将模型路径设置为 RetinaNet 模型的路径
    detector.loadModel()  # 模型加载到的目标检测类
    # 调用目标检测函数，解析输入的和输出的图像路径。
    detections = detector.detectObjectsFromImage(input_image=imgArray,
                                                 input_type='array',output_type='array')
    return detections

data = plt.imread('../img_classify/05-30.jpg')
model_path = ('../model/resnet50_coco_best_v2.1.0.h5')
t1 = time.time()
imgInfo = targetDetection(data,model_path)
t2 = time.time()
print(t2-t1)
plt.imshow(imgInfo[0])
plt.show()
```

### dlib（hog+svm）

首先使用dlib自带的image数据库标注工具制作数据集，然后通过dlib的官方代码对每一个待检测目标依次训练模型，最后将模型合并进行测试。

``` python

df = pd.DataFrame(
    100*np.ones((6, 6)),
    columns=["name", "x", "y", "w", "h", "confidences"],
)

detector1 = dlib.fhog_object_detector("../model/sz.svm")
detector2 = dlib.fhog_object_detector("../model/cz.svm")
detector3 = dlib.fhog_object_detector("../model/dp.svm")
detector4 = dlib.fhog_object_detector("../model/pb.svm")
detector5 = dlib.fhog_object_detector("../model/zb.svm")
detector6 = dlib.fhog_object_detector("../model/mf.svm")

detectors = [detector1, detector2,detector3, detector4,detector5, detector6]

[boxes, confidences, detector_idxs] = dlib.fhog_object_detector.run_multiple(detectors, image, upsample_num_times=1, adjust_threshold=0.0)

for i in range(len(boxes)):
    # print("detector {} found box {} with confidence {}.".format(detector_idxs[i], boxes[i], confidences[i]))
    df.iloc[i, 0] = detector_idxs[i]
    df.iloc[i, 1] = boxes[i].left()
    df.iloc[i, 2] = boxes[i].top()
    df.iloc[i, 3] = boxes[i].right() - boxes[i].left()
    df.iloc[i, 4] = boxes[i].bottom() - boxes[i].top()
    df.iloc[i, 5] = round(confidences[i], 6)

```
