---
title: 眼动追踪推荐眼镜二：软件篇
date: 2019-09-24 19:45:30
tags:
---

软件部分主要解决下面三个问题：
<<<<<<< HEAD
=======

>>>>>>> 057af1afbd2d1fd63b6b9c9578a8fc82136565b7
* 首先是人眼关键点的检测问题，通过多级级联的回归树进行眼眶和通孔的检测，得到关键点特征信息；  
* 其次是兴趣点的估计问题，通过梯度提升回归算法建立人眼关键点和兴趣点的映射关系；  
* 最后是兴趣点处的目标识别问题，通过yolo目标检测网络对检测视野中的待推荐目标，判断兴趣点是否落在识别框中。

<!-- more -->
<The rest of contents | 余下全文>

<<<<<<< HEAD

=======
>>>>>>> 057af1afbd2d1fd63b6b9c9578a8fc82136565b7
## 人眼关键点检测

首先使用人眼摄像头采集人眼图像数据，然后使用dlib自带的image数据库标注工具制作数据集，最后通过dlib的官方代码训练出模型，并对模型进行测试。

模型分别对眼眶和眼珠进行识别，并检测它们的位置。

* dlib train_object_detector  
<<<<<<< HEAD
=======

>>>>>>> 057af1afbd2d1fd63b6b9c9578a8fc82136565b7
http://dlib.net/train_object_detector.py.html

``` python
import os
import sys
import glob
import dlib

options = dlib.simple_object_detector_training_options()
# 单个眼睛不是左右对称的
# options.add_left_right_image_flips = True
# 支持向量机的C参数，通常默认取为5.自己适当更改参数以达到最好的效果
options.C = 5
# 线程数，你电脑有4核的话就填4
options.num_threads = 4
options.be_verbose = True

training_xml_path = "pupil.xml"
dlib.train_simple_object_detector(training_xml_path, "pupil.svm", options)
print("Training accuracy: {}".format(
    dlib.test_simple_object_detector(training_xml_path, "pupil.svm")))

training_xml_path = "eye.xml"
dlib.train_simple_object_detector(training_xml_path, "eye.svm", options)
print("Training accuracy: {}".format(
    dlib.test_simple_object_detector(training_xml_path, "eye.svm")))

```

<<<<<<< HEAD

=======
>>>>>>> 057af1afbd2d1fd63b6b9c9578a8fc82136565b7
## 兴趣点映射估计

分别采集人眼图像中眼眶和眼珠的坐标数据和视野图像中目标点的坐标数据，通过sklearn中的梯度提升回归算法建立人眼关键点数据和兴趣点数据的映射关系。  

``` python
# 读取数据
point = pd.read_csv("csv_data/points_p.csv")
world_x = point["world_x"]
world_y = point["world_y"]

# 读取数据中的标签列
eye = point[['eye_x', 'eye_y', 'pipil_x', 'pupil_y', 'pupil_w', 'pupil_h']]
print(eye)

clf = GBR(max_depth=10)
# clf = SGDR(loss='huber',penalty='l2',alpha=0.9,max_iter=1000)
# clf = KNeighborsRegressor(n_neighbors=20, weights="distance", algorithm="ball_tree", leaf_size=50)

clf.fit(eye, world_x)
joblib.dump(clf, "model/world_x.pkl")
print('得分：',clf.score(eye, world_x))

clf.fit(eye, world_y)
joblib.dump(clf, "model/world_y.pkl")
print('得分：',clf.score(eye, world_y))

```

## 目标检测

### yolov3

制作voc格式数据集，使用keras版本yolov3训练识别模型

### ImageAI

使用ImageAI提供的方法对视野图像进行目标检测。

``` python
# coding:utf-8
#  imageai下载地址：https://github.com/OlafenwaMoses/ImageAI
#  resnet50_coco_best_v2.1.0.h5 模型下载地址：https://github.com/fizyr/keras-retinanet/releases/
from imageai.Detection import ObjectDetection  # 导入了 ImageAI 目标检测类
import cv2
import os
import time
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import matplotlib.pyplot as plt

def targetDetection(imgArray,model_path):
    """
    :param imgArray: 图片数据，类型为ndarray
    :param model_path: retinanet模型路径
    :return:
    """
    path = os.path.abspath(model_path)
    detector = ObjectDetection()  # 定义了目标检测类
    detector.setModelTypeAsRetinaNet()  # 模型的类型设置为 RetinaNet
    detector.setModelPath(path)  # 将模型路径设置为 RetinaNet 模型的路径
    detector.loadModel()  # 模型加载到的目标检测类
    # 调用目标检测函数，解析输入的和输出的图像路径。
    detections = detector.detectObjectsFromImage(input_image=imgArray,
                                                 input_type='array',output_type='array')
    return detections

data = plt.imread('../img_classify/05-30.jpg')
model_path = ('../model/resnet50_coco_best_v2.1.0.h5')
t1 = time.time()
imgInfo = targetDetection(data,model_path)
t2 = time.time()
print(t2-t1)
plt.imshow(imgInfo[0])
plt.show()
```

### dlib（hog+svm）

首先使用dlib自带的image数据库标注工具制作数据集，然后通过dlib的官方代码对每一个待检测目标依次训练模型，最后将模型合并进行测试。

``` python

df = pd.DataFrame(
    100*np.ones((6, 6)),
    columns=["name", "x", "y", "w", "h", "confidences"],
)

detector1 = dlib.fhog_object_detector("../model/sz.svm")
detector2 = dlib.fhog_object_detector("../model/cz.svm")
detector3 = dlib.fhog_object_detector("../model/dp.svm")
detector4 = dlib.fhog_object_detector("../model/pb.svm")
detector5 = dlib.fhog_object_detector("../model/zb.svm")
detector6 = dlib.fhog_object_detector("../model/mf.svm")

detectors = [detector1, detector2,detector3, detector4,detector5, detector6]

[boxes, confidences, detector_idxs] = dlib.fhog_object_detector.run_multiple(detectors, image, upsample_num_times=1, adjust_threshold=0.0)

for i in range(len(boxes)):
    # print("detector {} found box {} with confidence {}.".format(detector_idxs[i], boxes[i], confidences[i]))
    df.iloc[i, 0] = detector_idxs[i]
    df.iloc[i, 1] = boxes[i].left()
    df.iloc[i, 2] = boxes[i].top()
    df.iloc[i, 3] = boxes[i].right() - boxes[i].left()
    df.iloc[i, 4] = boxes[i].bottom() - boxes[i].top()
    df.iloc[i, 5] = round(confidences[i], 6)

```
